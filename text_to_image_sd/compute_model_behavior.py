"""Compare images generated by two LoRA models."""
import argparse
import json
import os
import sys
import time

import numpy as np
import open_clip
import pandas as pd
import torch
from diffusers import DiffusionPipeline
from tqdm import tqdm

from src.aesthetics import get_aesthetic_model
from src.datasets import create_dataset
from src.utils import print_args


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Compare images generated by two LoRA models."
    )
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        default="lambdalabs/miniSD-diffusers",
        help="Path to pretrained model or model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        help="Revision of pretrained model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--variant",
        type=str,
        default=None,
        help=(
            "Variant of the model files of the pretrained model identifier from "
            "huggingface.co/models, 'e.g.' fp16"
        ),
    )
    parser.add_argument(
        "--lora_dir",
        type=str,
        default=None,
        help="directory containing LoRA weights to load",
    )
    parser.add_argument(
        "--lora_steps",
        type=int,
        default=None,
        help="number of steps the LoRA weights have been trained on",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=["artbench"],
        default="artbench",
        help="Dataset to determine which prompts to use for image generation",
    )
    parser.add_argument(
        "--removal_dist",
        type=str,
        help="distribution for removing data",
        choices=[
            "all",
            "uniform",
            "shapley",
            "datamodel",
            "loo",
            "aoi",
            "shapley_uniform",
            "percentile",
        ],
        default=None,
    )
    parser.add_argument(
        "--datamodel_alpha",
        type=float,
        help="alpha value for the datamodel removal distribution",
        default=None,
    )
    parser.add_argument(
        "--percentile_file",
        type=str,
        default=None,
        help="Path to JSON file containing percentile indices",
    )
    parser.add_argument(
        "--percentile_value",
        type=int,
        default=None,
        help="Percentile value (e.g., 1, 2, 3, 4, 5, 10, 15, 20, 30)",
    )
    parser.add_argument(
        "--percentile_type",
        type=str,
        default=None,
        choices=["top", "bottom"],
        help="Whether to use top or bottom percentile",
    )
    parser.add_argument(
        "--percentile_mode",
        type=str,
        default="remove",
        choices=["remove", "keep"],
        help="Whether to remove or keep the percentile players",
    )
    parser.add_argument(
        "--removal_seed",
        type=int,
        help="random seed for sampling from the removal distribution",
        default=0,
    )
    parser.add_argument(
        "--num_images",
        type=int,
        default=50,
        help="number of images to generate for computing model behaviors",
    )
    parser.add_argument(
        "--n_noises",
        type=int,
        default=3,
        help="number of noise samples per time step when calculating diffusion losses",
    )
    parser.add_argument(
        "--ckpt_freq",
        type=int,
        default=10,
        help="number of images before saving a checkpoint",
    )
    parser.add_argument(
        "--ckpt_path",
        type=str,
        default=None,
        help="filepath for saving the checkpoint",
    )
    parser.add_argument(
        "--img_dir",
        type=str,
        default=None,
        help="directory path for saving the generated images",
    )
    parser.add_argument(
        "--clean_up_ckpt",
        default=False,
        action="store_true",
        help="whether to clean up the checkpoint once the process is finished",
    )
    parser.add_argument(
        "--resolution",
        type=int,
        default=256,
        help="the resolution of generated image",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="seed for reproducible image generation",
    )
    parser.add_argument(
        "--db",
        type=str,
        help="path to database for saving the results",
        default=None,
        required=True,
    )
    parser.add_argument(
        "--exp_name",
        type=str,
        help="experiment name to save in the database",
        default=None,
        required=True,
    )
    parser.add_argument(
        "--no_duplicate",
        action="store_true",
        help="whether to avoid running a process that produces duplicate records",
    )
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=7.5,
    )
    return parser.parse_args()


def load_pipeline(args):
    """Load diffusion model pipeline."""
    pipeline = DiffusionPipeline.from_pretrained(
        args.pretrained_model_name_or_path,
        revision=args.revision,
        variant=args.variant,
    )
    pipeline.safety_checker = None
    pipeline.requires_safety_checker = False
    pipeline.set_progress_bar_config(disable=True)
    pipeline = pipeline.to("cuda")
    return pipeline


def main(args):
    """Main function."""
    # Check for duplicate record in the database.
    results_dict = vars(args)
    if args.db is not None and os.path.exists(args.db) and args.no_duplicate:
        df = pd.read_json(args.db, lines=True)
        query_dict = {
            key: val
            for key, val in results_dict.items()
            if key not in ["no_duplicate", "ckpt_freq"]  # Keys irrelevant to results.
        }
        for key, val in query_dict.items():
            if df.shape[0] == 0:
                has_record = False
                break
            if val is None:
                df = df[df[key].isna()]  # None is converted to NaN when saving.
            else:
                df = df[df[key] == val]
        has_record = df.shape[0] > 0
        if has_record:
            print(
                f"Found duplicate record in database at {args.db}. Process cancelled."
            )
            sys.exit(0)  # Exit without raising an error.

    # Load percentile indices if using percentile removal
    percentile_indices = None
    if args.removal_dist == "percentile":
        if args.percentile_file is None:
            raise ValueError(
                "percentile_file must be provided when using percentile removal"
            )
        if args.percentile_value is None or args.percentile_type is None:
            raise ValueError("percentile_value and percentile_type must be provided.")

        with open(args.percentile_file, "r") as f:
            percentile_data = json.load(f)

        key = f"{args.percentile_type}_{args.percentile_value}pct_indices"
        if key not in percentile_data:
            raise ValueError(f"Key '{key}' not found in percentile file")

        percentile_indices = percentile_data[key]
        print(
            f"Loaded {len(percentile_indices)} indices for {args.percentile_type} "
            f"{args.percentile_value}% from {args.percentile_file}"
        )

    dataset, _ = create_dataset(
        "artbench",
        train=True,
        removal_dist=args.removal_dist,
        removal_idx=args.removal_seed,
        datamodel_alpha=args.datamodel_alpha,
        percentile_indices=percentile_indices,
        percentile_mode=args.percentile_mode,
    )

    n = len(dataset["caption"])
    probs = torch.full((n,), 1.0 / n)
    g = torch.Generator().manual_seed(args.seed)
    idx = torch.multinomial(
        probs, num_samples=args.num_images, replacement=True, generator=g
    )
    prompt_list = [dataset["caption"][i] for i in idx.tolist()]

    pipeline = load_pipeline(args)

    if args.lora_dir is not None:
        weight_name = "pytorch_lora_weights"
        if args.lora_steps is not None:
            weight_name += f"_{args.lora_steps}"
        weight_name += ".safetensors"

        pipeline.unet.load_attn_procs(args.lora_dir, weight_name=weight_name)
        print("LoRA weights loaded from", os.path.join(args.lora_dir, weight_name))
    else:
        print("Pretrained model is loaded")

    generator = torch.Generator(device="cuda")
    generator.manual_seed(args.seed)
    noise_generator = torch.Generator(device="cuda")
    noise_generator.manual_seed(args.seed)

    aesthetic_score_list, aesthetic_score_time_list = [], []

    num_completed_images = 0

    # Load the aesthetic model and the corresponding CLIP for aesthetic scoring.
    aesthetic_model = get_aesthetic_model(clip_model="vit_l_14")
    aesthetic_model = aesthetic_model.to("cuda")
    (
        open_clip_model,
        _,
        open_clip_preprocess,
    ) = open_clip.create_model_and_transforms("ViT-L-14", pretrained="openai")
    open_clip_model = open_clip_model.to("cuda")

    # Load existing checkpoint.
    if args.ckpt_path is not None and os.path.exists(args.ckpt_path):
        ckpt = torch.load(args.ckpt_path)
        generator.set_state(ckpt["generator_state"])
        noise_generator.set_state(ckpt["noise_generator_state"])

        aesthetic_score_list = ckpt["aesthetic_score_list"]
        aesthetic_score_time_list = ckpt["aesthetic_score_time_list"]

        num_completed_images = ckpt["num_completed_images"]
        print(f"Checkpoint loaded from {args.ckpt_path}")

    # Compute model behaviors.
    progress_bar = tqdm(initial=num_completed_images, total=args.num_images)
    for i in range(num_completed_images, args.num_images):
        generation_time = time.time()
        img = pipeline(
            prompt_list[i],
            num_inference_steps=100,
            generator=generator,
            height=args.resolution,
            width=args.resolution,
            guidance_scale=args.guidance_scale,
        ).images[0]
        generation_time = time.time() - generation_time

        if args.img_dir is not None:
            img.save(os.path.join(args.img_dir, f"img_seed={args.seed}_sample_{i}.jpg"))

        # Aesthetic score.
        aesthetic_score_time_list.append(time.time())

        with torch.no_grad():
            open_clip_embedding = open_clip_model.encode_image(
                open_clip_preprocess(img).to("cuda").unsqueeze(0)
            )
            open_clip_embedding /= open_clip_embedding.norm(dim=-1, keepdim=True)
            aesthetic_score = aesthetic_model(open_clip_embedding)
            aesthetic_score_list.append(aesthetic_score.item())
        aesthetic_score_time_list[-1] = (
            time.time() - aesthetic_score_time_list[-1] + generation_time
        )

        num_completed_images += 1
        progress_bar.update(1)

        if (args.ckpt_path is not None) and (
            num_completed_images % args.ckpt_freq == 0
        ):
            ckpt = {
                "generator_state": generator.get_state(),
                "noise_generator_state": noise_generator.get_state(),
                "aesthetic_score_list": aesthetic_score_list,
                "aesthetic_score_time_list": aesthetic_score_time_list,
                "num_completed_images": num_completed_images,
            }
            torch.save(ckpt, args.ckpt_path)
            print(f"Checkpoint saved tp {args.ckpt_path}")

    # Save results to the database.
    if args.db is not None:
        # Local model behaviors.
        for i in range(num_completed_images):
            prefix = f"generated_image_{i}"
            results_dict[prefix + "_aesthetic_score"] = aesthetic_score_list[i]
            results_dict[prefix + "_aesthetic_score_time"] = aesthetic_score_time_list[
                i
            ]

        # Global model behaviors.
        for q in [0.5, 0.75, 0.9]:
            results_dict[f"aesthetic_score_{q}"] = np.quantile(
                aesthetic_score_list, q=q
            )
        results_dict["aesthetic_score_avg"] = np.mean(aesthetic_score_list)
        results_dict["aesthetic_score_time"] = np.sum(aesthetic_score_time_list)
        results_dict["removal_seed"] = args.removal_seed
        results_dict["removal_dist"] = args.removal_dist
        results_dict["datamodel_alpha"] = args.datamodel_alpha
        results_dict["guidance_scale"] = args.guidance_scale

        # Add percentile metadata if using percentile removal
        if args.removal_dist == "percentile":
            results_dict["percentile_value"] = args.percentile_value
            results_dict["percentile_type"] = args.percentile_type
            results_dict["percentile_mode"] = args.percentile_mode

        # results_dict["remaining_idx"] = remaining_idx.tolist()
        # results_dict["removal_idx"] = removed_idx.tolist()

        with open(args.db, "a+") as f:
            f.write(json.dumps(results_dict) + "\n")
        print(f"Results saved to the database at {args.db}")

    if (
        args.ckpt_path is not None
        and os.path.exists(args.ckpt_path)
        and args.clean_up_ckpt
    ):
        os.remove(args.ckpt_path)
        print(f"Checkpoint at {args.ckpt_path} removed")
    print("Done!")


if __name__ == "__main__":
    args = parse_args()
    print_args(args)
    main(args)
