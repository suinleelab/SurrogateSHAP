"""Script to compute TRAK/D-TRAK/Journey-TRAK gradient for Fashion100 dataset."""
import argparse
import os

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from diffusers import FluxPipeline
from torch.func import functional_call, grad, vmap
from tqdm import tqdm
from trak.projectors import CudaProjector, ProjectionType
from trak.utils import is_not_buffer

from src.constants import DATASET_DIR
from src.datasets import FashionDatasetWrapper, create_dataset


def count_parameters(model):
    """Count the number of parameters requiring gradients."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Compare images generated by two LoRA models."
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        required=True,
        help="The output directory.",
    )
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        default="black-forest-labs/FLUX.1-dev",
        help="Path to pretrained model or model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--with_prior_preservation",
        default=False,
        action="store_true",
        help="Flag to add prior preservation loss.",
    )
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        help="Revision of pretrained model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--variant",
        type=str,
        default=None,
        help=(
            "Variant of the model files of the pretrained model identifier from "
            "huggingface.co/models, 'e.g.' fp16"
        ),
    )
    parser.add_argument(
        "--lora_dir",
        type=str,
        default=None,
        help="directory containing LoRA weights to load",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=["fashion"],
        default="fashion",
        help="Dataset to determine which prompts to use for image generation",
    )
    parser.add_argument(
        "--source",
        type=str,
        default="train",
        choices=["train", "generated", "generated_journey"],
        help="source of data for computing gradients",
    )
    parser.add_argument(
        "--train_batch_size",
        type=int,
        default=8,
        help="batch size for compute grad, minimum is 8 for fast_trak",
    )
    parser.add_argument(
        "--dataloader_num_workers",
        type=int,
        default=0,
        help=(
            "Number of subprocesses to use for data loading. "
            "0 means that the data will be loaded in the main process."
        ),
    )
    parser.add_argument(
        "--num_images",
        type=int,
        default=50,
        help="number of images to generate for computing model behaviors",
    )
    parser.add_argument(
        "--n_noises",
        type=int,
        default=3,
        help="number of noise samples per time step when calculating diffusion losses",
    )
    parser.add_argument(
        "--resolution",
        type=int,
        default=256,
        help="the resolution of generated image",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="seed for reproducible image generation",
    )
    parser.add_argument(
        "--no_duplicate",
        action="store_true",
        help="whether to avoid running a process that produces duplicate records",
    )
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=3.5,
    )
    parser.add_argument(
        "--generation_seed",
        type=int,
        default=42,
        help="seed for image generation",
    )
    parser.add_argument(
        "--num_journey_points",
        type=int,
        default=50,
        help="number of time points selected for Journey-TRAK",
    )
    parser.add_argument(
        "--num_journey_noises",
        type=int,
        default=1,
        help="number of noises to sample for intermediate latent at each time step",
    )
    parser.add_argument(
        "--num_timesteps",
        type=int,
        help="number of timesteps to select for computing gradients",
        default=100,
    )
    parser.add_argument(
        "--proj_dim",
        type=int,
        help="dimension size for projected gradients",
        default=32768,
    )
    parser.add_argument(
        "--f",
        type=str,
        help="loss function for computing gradients",
        required=True,
    )
    return parser.parse_args()


def main(args):
    """Main function to compute gradient embeddings for Fashion100 dataset."""
    # Initialize accelerator
    if args.output_dir is not None:
        args.output_dir = os.path.join(
            args.output_dir, "fashion", "gradients", args.source
        )
        os.makedirs(args.output_dir, exist_ok=True)

    weight_dtype = torch.float16

    # Load pipeline
    pipeline = FluxPipeline.from_pretrained(
        args.pretrained_model_name_or_path,
        revision=args.revision,
        variant=args.variant,
        torch_dtype=weight_dtype,
    )

    # Load LoRA weights if provided
    if args.lora_dir is not None:
        pipeline.load_lora_weights(
            args.lora_dir, weight_name="pytorch_lora_weights.safetensors"
        )
        print(f"Load LORA from {args.lora_dir}")

    pipeline = pipeline.to("cuda")
    pipeline.set_progress_bar_config(disable=True)

    if args.source == "train":
        # Load training dataset and sample prompts

        def collate_fn(examples, with_prior_preservation=False):
            pixel_values = [example["instance_images"] for example in examples]
            prompts = [example["instance_prompt"] for example in examples]
            filenames = [example["filename"] for example in examples]

            # Concat class and instance examples for prior preservation.
            # We do this to avoid doing two forward passes.
            if with_prior_preservation:
                pixel_values += [example["class_images"] for example in examples]
                prompts += [example["class_prompt"] for example in examples]

            pixel_values = torch.stack(pixel_values)
            pixel_values = pixel_values.to(
                memory_format=torch.contiguous_format
            ).float()

            batch = {
                "pixel_values": pixel_values,
                "prompts": prompts,
                "filenames": filenames,
            }
            return batch

        train_dataset, _ = create_dataset(
            "fashion",
            train=True,
        )
        fashion_dataset = FashionDatasetWrapper(
            hf_dataset=train_dataset,
            size=args.resolution,
            pad_to_square=True,
            custom_instance_prompts=True,
        )
        train_dataloader = torch.utils.data.DataLoader(
            fashion_dataset,
            batch_size=args.train_batch_size,
            shuffle=False,
            collate_fn=lambda examples: collate_fn(
                examples, args.with_prior_preservation
            ),
            num_workers=args.dataloader_num_workers,
        )
        # load cache embeddings
        vqvae_latent_dir = os.path.join(
            DATASET_DIR,
            "fashion-product",
            "precomputed_emb",
        )
        latents_cache = torch.load(
            os.path.join(vqvae_latent_dir, "vqvae_output.pt"),
        )
        print("VQVAE output loaded.")

        text_embeddings_dir = os.path.join(
            DATASET_DIR,
            "fashion-product",
            "precomputed_text_emb",
        )
        text_embeddings_cache = torch.load(
            os.path.join(text_embeddings_dir, "text_embeddings.pt"),
        )
        print("Text embeddings loaded.")

        # Get training data latents and text encoder hidden states.
        (
            all_latents,
            all_encoder_hidden_states,
            all_pooled_prompt_embeds,
            all_text_ids,
        ) = ([], [], [], [])

        for batch in train_dataloader:
            filenames = batch["filenames"]

            prompt_embeds = torch.stack(
                [
                    text_embeddings_cache[filename]["prompt_embeds"]
                    for filename in filenames
                ]
            ).to("cuda")
            pooled_prompt_embeds = torch.stack(
                [
                    text_embeddings_cache[filename]["pooled_prompt_embeds"]
                    for filename in filenames
                ]
            ).to("cuda")

            latents = torch.stack(
                [latents_cache[filename] for filename in filenames]
            ).to("cuda", dtype=weight_dtype)

            all_latents.append(latents.detach().cpu())
            all_encoder_hidden_states.append(prompt_embeds.detach().cpu())
            all_pooled_prompt_embeds.append(pooled_prompt_embeds.detach().cpu())

        all_latents = torch.cat(all_latents)
        all_encoder_hidden_states = torch.cat(all_encoder_hidden_states)
        all_pooled_prompt_embeds = torch.cat(all_pooled_prompt_embeds)
        # Create text_ids based on prompt_embeds shape, similar to train_lora_flux.py
        all_text_ids = torch.zeros(
            all_encoder_hidden_states.shape[1], 3, dtype=weight_dtype
        )

        group_df = pd.DataFrame(
            {
                "index": [i for i in range(len(train_dataset))],
                "brand": train_dataset["brand"],
                "filename": train_dataset["filename"],
            }
        )
        group_df.to_csv(os.path.join(args.output_dir, "group.csv"), index=False)

    else:
        pipeline = FluxPipeline.from_pretrained(
            args.pretrained_model_name_or_path,
            revision=args.revision,
            variant=args.variant,
            torch_dtype=weight_dtype,
        )

        # Load LoRA weights if provided
        if args.lora_dir is not None:
            pipeline.load_lora_weights(
                args.lora_dir, weight_name="pytorch_lora_weights.safetensors"
            )
            print(f"Load LORA from {args.lora_dir}")

        pipeline = pipeline.to("cuda")
        pipeline.set_progress_bar_config(disable=True)
        dataset, _ = create_dataset(
            "fashion",
            train=True,
            removal_dist="all",
        )

        # Sample prompts deterministically
        n = len(dataset["prompt"])
        if n == 0:
            raise ValueError("Dataset returned 0 captions")

        probs = torch.full((n,), 1.0 / n)
        g = torch.Generator().manual_seed(args.seed)
        idx = torch.multinomial(
            probs, num_samples=args.num_images, replacement=True, generator=g
        )
        prompt_list = [dataset["prompt"][i] for i in idx.tolist()]
        all_step_idx, all_t, all_latents, all_generated_image_idx = [], [], [], []
        all_encoder_hidden_states, all_pooled_prompt_embeds = [], []

        print("Obtaining latents for generated images...")

        for i in tqdm(range(args.num_images)):
            seed_per_image = args.seed + i
            generator = torch.Generator(device="cuda").manual_seed(seed_per_image)

            # Pre-calculate prompt embeds
            with torch.no_grad():
                prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(
                    prompt_list[i], prompt_2=prompt_list[i]
                )

                # Initialize latents
                num_inference_steps = args.num_timesteps
                latent_shape = (1, 16, args.resolution // 8, args.resolution // 8)
                latents = torch.randn(
                    latent_shape, generator=generator, device="cuda", dtype=weight_dtype
                )

                # Calculate mu for dynamic shifting (required by FLUX scheduler)
                image_seq_len = (args.resolution // 8) * (args.resolution // 8)

                # Use the scheduler's time_shift method directly
                # Calculate mu based on image sequence length
                base_seq_len = pipeline.scheduler.config.base_image_seq_len
                max_seq_len = pipeline.scheduler.config.max_image_seq_len
                base_shift = pipeline.scheduler.config.base_shift
                max_shift = pipeline.scheduler.config.max_shift

                # Linear interpolation between base_shift and max_shift
                mu = base_shift + (max_shift - base_shift) * (
                    image_seq_len - base_seq_len
                ) / (max_seq_len - base_seq_len)
                mu = max(base_shift, min(max_shift, mu))

                # Get scheduler timesteps
                pipeline.scheduler.set_timesteps(
                    num_inference_steps, device="cuda", mu=mu
                )
                timesteps = pipeline.scheduler.timesteps

                # Prepare latent image IDs
                img_ids = pipeline._prepare_latent_image_ids(
                    latents.shape[0],
                    latents.shape[2] // 2,
                    latents.shape[3] // 2,
                    "cuda",
                    weight_dtype,
                )

                # Manual denoising loop to collect intermediate latents
                step_idx_list, t_list, latents_list = [], [], []

                for step_idx, t in enumerate(timesteps):
                    step_idx_list.append(step_idx)
                    t_list.append(t.detach().cpu())
                    latents_list.append(latents.detach().cpu())

                    # Pack latents
                    latent_model_input = pipeline._pack_latents(
                        latents,
                        batch_size=1,
                        num_channels_latents=16,
                        height=latents.shape[2],
                        width=latents.shape[3],
                    )

                    # Predict noise
                    timestep = t.expand(latents.shape[0])
                    guidance = torch.tensor(
                        [args.guidance_scale], device="cuda", dtype=weight_dtype
                    )

                    noise_pred = pipeline.transformer(
                        hidden_states=latent_model_input,
                        timestep=timestep / 1000,
                        guidance=guidance,
                        pooled_projections=pooled_prompt_embeds,
                        encoder_hidden_states=prompt_embeds,
                        txt_ids=text_ids,
                        img_ids=img_ids,
                        return_dict=False,
                    )[0]

                    # Unpack
                    noise_pred = pipeline._unpack_latents(
                        noise_pred,
                        height=args.resolution,
                        width=args.resolution,
                        vae_scale_factor=8,
                    )

                    # Scheduler step
                    latents = pipeline.scheduler.step(
                        noise_pred, t, latents, return_dict=False
                    )[0]

            generated_image_idx_list = [i] * len(step_idx_list)

            if args.source == "generated":
                # Collect only the final latent variable
                all_step_idx.append(step_idx_list[-1])
                all_t.append(t_list[-1])
                all_latents.append(latents_list[-1])
                all_generated_image_idx.append(generated_image_idx_list[-1])
                all_encoder_hidden_states.append(prompt_embeds.detach().cpu())
                all_pooled_prompt_embeds.append(pooled_prompt_embeds.detach().cpu())

            elif args.source == "generated_journey":
                num_inference_steps = len(step_idx_list)
                for j in np.arange(
                    start=1,
                    stop=num_inference_steps,
                    step=num_inference_steps // args.num_journey_points,
                ):
                    all_step_idx.append(step_idx_list[j])
                    all_t.append(t_list[j])
                    all_latents.append(latents_list[j])
                    all_generated_image_idx.append(generated_image_idx_list[j])
                    all_encoder_hidden_states.append(prompt_embeds.detach().cpu())
                    all_pooled_prompt_embeds.append(pooled_prompt_embeds.detach().cpu())
            else:
                raise NotImplementedError

        group_df = pd.DataFrame(
            {"generated_image_idx": all_generated_image_idx, "step_idx": all_step_idx}
        )
        group_df.to_csv(os.path.join(args.output_dir, "group.csv"), index=True)

        all_latents = torch.cat(all_latents).to(weight_dtype)
        all_t = torch.stack(all_t)
        all_encoder_hidden_states = torch.cat(all_encoder_hidden_states)
        all_pooled_prompt_embeds = torch.cat(all_pooled_prompt_embeds)
        # Create text_ids based on prompt_embeds shape, similar to train_lora_flux.py
        all_text_ids = torch.zeros(
            all_encoder_hidden_states.shape[1], 3, dtype=weight_dtype
        )

    if args.source == "generated_journey":
        dataloader = torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(
                all_latents, all_t, all_encoder_hidden_states, all_pooled_prompt_embeds
            ),
            shuffle=False,  # Do not turn on shuffle to keep the group mapping intact!
            batch_size=args.train_batch_size,
            num_workers=args.dataloader_num_workers,
            pin_memory=True,
        )
    else:
        dataloader = torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(
                all_latents, all_encoder_hidden_states, all_pooled_prompt_embeds
            ),
            shuffle=False,  # Do not turn on shuffle to keep the group mapping intact!
            batch_size=args.train_batch_size,
            num_workers=args.dataloader_num_workers,
            pin_memory=True,
        )
    model = pipeline.transformer
    model.eval()
    model.requires_grad_(False)

    # Enable gradients only for LoRA parameters
    if args.lora_dir is not None:
        print("\nEnabling gradients for LoRA parameters...")
        lora_params_count = 0
        for param_name, param in model.named_parameters():
            if "lora" in param_name.lower():
                param.requires_grad_(True)
                lora_params_count += 1
                if lora_params_count <= 10:  # Show first 10
                    print(f"  -> {param_name}")

        print(f"\nTotal LoRA parameters enabled: {lora_params_count}")
        print(f"Parameters with requires_grad=True: {count_parameters(model)}")
    else:
        print("\nNo LoRA directory specified, all parameters frozen.")

    projector = CudaProjector(
        grad_dim=count_parameters(model),
        proj_dim=args.proj_dim,
        seed=args.seed,
        proj_type=ProjectionType.normal,
        device="cuda",
        max_batch_size=args.train_batch_size,
    )

    params = {k: v.detach() for k, v in model.named_parameters() if v.requires_grad}
    buffers = {k: v.detach() for k, v in model.named_buffers() if v.requires_grad}

    def vectorize_and_ignore_buffers(g, params_dict=None):
        """
        Gradients are given as a tuple :code:`(grad_w0, grad_w1, ... grad_wp)` where
        :code:`p` is the number of weight matrices. each :code:`grad_wi` has shape
        :code:`[batch_size, ...]` this function flattens :code:`g` to have shape
        :code:`[batch_size, num_params]`.
        """
        batch_size = len(g[0])
        out = []
        if params_dict is not None:
            for b in range(batch_size):
                out.append(
                    torch.cat(
                        [
                            x[b].flatten()
                            for i, x in enumerate(g)
                            if is_not_buffer(i, params_dict)
                        ]
                    )
                )
        else:
            for b in range(batch_size):
                out.append(torch.cat([x[b].flatten() for x in g]))
        return torch.stack(out)

    if args.f == "mean-squared-l2-norm":
        print(args.f)

        def compute_f(
            params,
            buffers,
            packed_noisy_latents,
            timesteps,
            encoder_hidden_states,
            pooled_prompt_embeds,
            text_ids,
            img_ids,
            guidance,
            targets,
        ):
            packed_noisy_latents = packed_noisy_latents.unsqueeze(0)
            timesteps = timesteps.unsqueeze(0)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(0)
            pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)
            # text_ids and img_ids should remain 2D (no batch dimension)
            guidance = guidance.unsqueeze(0)
            targets = targets.unsqueeze(0)

            predictions = functional_call(
                model,
                (params, buffers),
                kwargs={
                    "hidden_states": packed_noisy_latents,
                    "timestep": timesteps / 1000,
                    "guidance": guidance,
                    "encoder_hidden_states": encoder_hidden_states,
                    "pooled_projections": pooled_prompt_embeds,
                    "txt_ids": text_ids,
                    "img_ids": img_ids,
                },
            )
            predictions = predictions.sample
            ####
            # predictions = predictions.reshape(1, -1)
            # f = torch.norm(predictions.float(), p=2.0, dim=-1)**2 # squared
            # f = f/predictions.size(1) # mean
            # f = f.mean()
            ####
            f = F.mse_loss(
                predictions.float(), torch.zeros_like(targets).float(), reduction="none"
            )
            f = f.reshape(1, -1)
            f = f.mean()
            ####
            # print(f.size())
            # print(f)
            ####
            return f

    elif args.f == "mean":
        print(args.f)

        def compute_f(
            params,
            buffers,
            packed_noisy_latents,
            timesteps,
            encoder_hidden_states,
            pooled_prompt_embeds,
            text_ids,
            img_ids,
            guidance,
            targets,
        ):
            packed_noisy_latents = packed_noisy_latents.unsqueeze(0)
            timesteps = timesteps.unsqueeze(0)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(0)
            pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)
            # text_ids and img_ids should remain 2D (no batch dimension)
            guidance = guidance.unsqueeze(0)
            targets = targets.unsqueeze(0)

            predictions = functional_call(
                model,
                (params, buffers),
                kwargs={
                    "hidden_states": packed_noisy_latents,
                    "timestep": timesteps / 1000,
                    "guidance": guidance,
                    "encoder_hidden_states": encoder_hidden_states,
                    "pooled_projections": pooled_prompt_embeds,
                    "txt_ids": text_ids,
                    "img_ids": img_ids,
                },
            )
            predictions = predictions.sample
            ####
            f = predictions.float()
            f = f.reshape(1, -1)
            f = f.mean()
            ####
            # print(f.size())
            # print(f)
            ####
            return f

    elif args.f == "l1-norm":
        print(args.f)

        def compute_f(
            params,
            buffers,
            packed_noisy_latents,
            timesteps,
            encoder_hidden_states,
            pooled_prompt_embeds,
            text_ids,
            img_ids,
            guidance,
            targets,
        ):
            packed_noisy_latents = packed_noisy_latents.unsqueeze(0)
            timesteps = timesteps.unsqueeze(0)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(0)
            pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)
            # text_ids and img_ids should remain 2D (no batch dimension)
            guidance = guidance.unsqueeze(0)
            targets = targets.unsqueeze(0)

            predictions = functional_call(
                model,
                (params, buffers),
                kwargs={
                    "hidden_states": packed_noisy_latents,
                    "timestep": timesteps / 1000,
                    "guidance": guidance,
                    "encoder_hidden_states": encoder_hidden_states,
                    "pooled_projections": pooled_prompt_embeds,
                    "txt_ids": text_ids,
                    "img_ids": img_ids,
                },
            )
            predictions = predictions.sample
            ####
            predictions = predictions.reshape(1, -1)
            f = torch.norm(predictions.float(), p=1.0, dim=-1)
            f = f.mean()
            ####
            # print(f.size())
            # print(f)
            ####
            return f

    elif args.f == "l2-norm":
        print(args.f)

        def compute_f(
            params,
            buffers,
            packed_noisy_latents,
            timesteps,
            encoder_hidden_states,
            pooled_prompt_embeds,
            text_ids,
            img_ids,
            guidance,
            targets,
        ):
            packed_noisy_latents = packed_noisy_latents.unsqueeze(0)
            timesteps = timesteps.unsqueeze(0)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(0)
            pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)
            # text_ids and img_ids should remain 2D (no batch dimension)
            guidance = guidance.unsqueeze(0)
            targets = targets.unsqueeze(0)

            predictions = functional_call(
                model,
                (params, buffers),
                kwargs={
                    "hidden_states": packed_noisy_latents,
                    "timestep": timesteps / 1000,
                    "guidance": guidance,
                    "encoder_hidden_states": encoder_hidden_states,
                    "pooled_projections": pooled_prompt_embeds,
                    "txt_ids": text_ids,
                    "img_ids": img_ids,
                },
            )
            predictions = predictions.sample
            ####
            predictions = predictions.reshape(1, -1)
            f = torch.norm(predictions.float(), p=2.0, dim=-1)
            f = f.mean()
            ####
            # print(f.size())
            # print(f)
            ####
            return f

    elif args.f == "linf-norm":
        print(args.f)

        def compute_f(
            params,
            buffers,
            packed_noisy_latents,
            timesteps,
            encoder_hidden_states,
            pooled_prompt_embeds,
            text_ids,
            img_ids,
            guidance,
            targets,
        ):
            packed_noisy_latents = packed_noisy_latents.unsqueeze(0)
            timesteps = timesteps.unsqueeze(0)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(0)
            pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)
            # text_ids and img_ids should remain 2D (no batch dimension)
            guidance = guidance.unsqueeze(0)
            targets = targets.unsqueeze(0)

            predictions = functional_call(
                model,
                (params, buffers),
                kwargs={
                    "hidden_states": packed_noisy_latents,
                    "timestep": timesteps / 1000,
                    "guidance": guidance,
                    "encoder_hidden_states": encoder_hidden_states,
                    "pooled_projections": pooled_prompt_embeds,
                    "txt_ids": text_ids,
                    "img_ids": img_ids,
                },
            )
            predictions = predictions.sample
            ####
            predictions = predictions.reshape(1, -1)
            f = torch.norm(predictions.float(), p=float("inf"), dim=-1)
            f = f.mean()
            ####
            # print(f.size())
            # print(f)
            ####
            return f

    else:
        print(args.f)

        def compute_f(
            params,
            buffers,
            packed_noisy_latents,
            timesteps,
            encoder_hidden_states,
            pooled_prompt_embeds,
            text_ids,
            img_ids,
            guidance,
            targets,
        ):
            packed_noisy_latents = packed_noisy_latents.unsqueeze(0)
            timesteps = timesteps.unsqueeze(0)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(0)
            pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)
            # text_ids and img_ids should remain 2D (no batch dimension)
            guidance = guidance.unsqueeze(0)
            targets = targets.unsqueeze(0)

            predictions = functional_call(
                model,
                (params, buffers),
                kwargs={
                    "hidden_states": packed_noisy_latents,
                    "timestep": timesteps / 1000,
                    "guidance": guidance,
                    "encoder_hidden_states": encoder_hidden_states,
                    "pooled_projections": pooled_prompt_embeds,
                    "txt_ids": text_ids,
                    "img_ids": img_ids,
                },
            )
            predictions = predictions.sample
            ####
            f = F.mse_loss(predictions.float(), targets.float(), reduction="none")
            f = f.reshape(1, -1)
            f = f.mean()
            ####
            return f

    ft_compute_grad = grad(compute_f)
    ft_compute_sample_grad = vmap(
        ft_compute_grad,
        in_dims=(None, None, 0, 0, 0, 0, None, None, 0, 0),
    )
    # FLUX uses 'scheduler' instead of 'noise_scheduler'
    noise_scheduler = pipeline.scheduler

    if args.source == "generated_journey":
        all_embs = []
        for (latents, timesteps, encoder_hidden_states, pooled_prompt_embeds) in tqdm(
            dataloader
        ):
            latents = latents.to("cuda", dtype=weight_dtype)
            timesteps = timesteps.to("cuda")
            encoder_hidden_states = encoder_hidden_states.to("cuda", dtype=weight_dtype)
            pooled_prompt_embeds = pooled_prompt_embeds.to("cuda", dtype=weight_dtype)

            for index_noise in range(args.num_journey_noises):
                noise = torch.randn_like(latents)
                # FLUX uses flow matching: noisy_latents
                #  = (1 - sigma) * latents + sigma * noise
                # Convert timesteps to sigmas
                sigmas = timesteps.float() / noise_scheduler.config.num_train_timesteps
                sigmas = sigmas.view(-1, 1, 1, 1)
                noisy_latents = (1 - sigmas) * latents + sigmas * noise

                # Pack latents for FLUX transformer
                packed_noisy_latents = FluxPipeline._pack_latents(
                    noisy_latents,
                    batch_size=noisy_latents.shape[0],
                    num_channels_latents=noisy_latents.shape[1],
                    height=noisy_latents.shape[2],
                    width=noisy_latents.shape[3],
                ).to(weight_dtype)

                # Prepare img_ids for FLUX transformer
                img_ids = FluxPipeline._prepare_latent_image_ids(
                    noisy_latents.shape[0],
                    noisy_latents.shape[2] // 2,
                    noisy_latents.shape[3] // 2,
                    noisy_latents.device,
                    noisy_latents.dtype,
                )

                # For flow matching, the target is the velocity (noise - latents)
                target = noise - latents

                packed_target = FluxPipeline._pack_latents(
                    target,
                    batch_size=target.shape[0],
                    num_channels_latents=target.shape[1],
                    height=target.shape[2],
                    width=target.shape[3],
                ).to(weight_dtype)

                # Create guidance tensor (constant for all samples)
                guidance = torch.full(
                    (latents.shape[0],),
                    args.guidance_scale,
                    device=latents.device,
                    dtype=weight_dtype,
                )

                ft_per_sample_grads = ft_compute_sample_grad(
                    params,
                    buffers,
                    packed_noisy_latents,
                    timesteps,
                    encoder_hidden_states,
                    pooled_prompt_embeds,
                    all_text_ids.to("cuda"),
                    img_ids,
                    guidance,
                    packed_target,
                )

                ft_per_sample_grads = vectorize_and_ignore_buffers(
                    list(ft_per_sample_grads.values())
                )
                if index_noise == 0:
                    emb = ft_per_sample_grads
                else:
                    emb += ft_per_sample_grads
            emb = emb / args.num_journey_noises
            emb = projector.project(emb, model_id=0)
            all_embs.append(emb.detach().cpu())

        all_embs = torch.cat(all_embs)
        output_filename = (
            f"emb_f={args.f}_num_journey_points={args.num_journey_points}"
            f"_num_journey_noises={args.num_journey_noises}"
            f"_proj_dim={args.proj_dim}.pt"
        )
        torch.save(all_embs, os.path.join(args.output_dir, output_filename))
    else:
        all_embs = []
        for (latents, encoder_hidden_states, pooled_prompt_embeds) in tqdm(dataloader):
            latents = latents.to("cuda", dtype=weight_dtype)
            encoder_hidden_states = encoder_hidden_states.to("cuda", dtype=weight_dtype)
            pooled_prompt_embeds = pooled_prompt_embeds.to("cuda", dtype=weight_dtype)

            bsz = latents.shape[0]
            selected_timesteps = range(0, 1000, 1000 // args.num_timesteps)

            for index_t, t in enumerate(selected_timesteps):
                timesteps = torch.tensor([t] * bsz, device=latents.device)
                timesteps = timesteps.long()

                noise = torch.randn_like(latents)
                # FLUX uses flow matching: noisy_latents
                # = (1 - sigma) * latents + sigma * noise
                # Convert timesteps to sigmas
                sigmas = timesteps.float() / noise_scheduler.config.num_train_timesteps
                sigmas = sigmas.view(-1, 1, 1, 1)
                noisy_latents = (1 - sigmas) * latents + sigmas * noise

                # Pack latents for FLUX transformer
                packed_noisy_latents = FluxPipeline._pack_latents(
                    noisy_latents,
                    batch_size=noisy_latents.shape[0],
                    num_channels_latents=noisy_latents.shape[1],
                    height=noisy_latents.shape[2],
                    width=noisy_latents.shape[3],
                ).to(weight_dtype)

                # Prepare img_ids for FLUX transformer
                img_ids = FluxPipeline._prepare_latent_image_ids(
                    noisy_latents.shape[0],
                    noisy_latents.shape[2] // 2,
                    noisy_latents.shape[3] // 2,
                    noisy_latents.device,
                    noisy_latents.dtype,
                )

                # For flow matching, the target is the velocity (noise - latents)
                target = noise - latents
                packed_target = FluxPipeline._pack_latents(
                    target,
                    batch_size=target.shape[0],
                    num_channels_latents=target.shape[1],
                    height=target.shape[2],
                    width=target.shape[3],
                ).to(weight_dtype)

                # Create guidance tensor (constant for all samples)
                guidance = torch.full(
                    (bsz,),
                    args.guidance_scale,
                    device=latents.device,
                    dtype=weight_dtype,
                )

                ft_per_sample_grads = ft_compute_sample_grad(
                    params,
                    buffers,
                    packed_noisy_latents,
                    timesteps,
                    encoder_hidden_states,
                    pooled_prompt_embeds,
                    all_text_ids.to("cuda"),
                    img_ids,
                    guidance,
                    packed_target,
                )

                ft_per_sample_grads = vectorize_and_ignore_buffers(
                    list(ft_per_sample_grads.values())
                )
                if index_t == 0:
                    emb = ft_per_sample_grads
                else:
                    emb += ft_per_sample_grads
            emb = emb / args.num_timesteps
            emb = projector.project(emb, model_id=0)
            all_embs.append(emb.detach().cpu())

        all_embs = torch.cat(all_embs)
        output_filename = (
            f"emb_f={args.f}_num_timesteps={args.num_timesteps}"
            f"_proj_dim={args.proj_dim}.pt"
        )
        torch.save(all_embs, os.path.join(args.output_dir, output_filename))


if __name__ == "__main__":
    args = parse_args()
    main(args)
    print("Done!")
