"""Script that compute fashion related metrics for fashion dataset"""
import argparse
import json
import os
from pathlib import Path

import lpips
import numpy as np
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from diffusers import FluxPipeline
from filelock import FileLock
from PIL import Image
from pytorch_msssim import ms_ssim
from torchvision import models, transforms
from tqdm import tqdm

from src.attributions.global_scores.diversity_score import calculate_diversity_score
from src.constants import DATASET_DIR
from src.datasets import create_dataset


def pil_to_lpips_tensor(img):
    """Convert PIL image to tensor for LPIPS (range [-1, 1])"""
    transform = transforms.Compose(
        [
            transforms.Resize((args.resolution, args.resolution)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        ]
    )
    return transform(img).unsqueeze(0)


def pil_to_inception_tensor(img, size=299):
    """Convert PIL image to tensor for Inception (range [0, 1])"""
    transform = transforms.Compose(
        [
            transforms.Resize((size, size)),
            transforms.ToTensor(),
        ]
    )
    return transform(img)


def pil_to_msssim_tensor(img, size=256):
    """Convert PIL image to tensor for MS-SSIM (range [0, 1])"""
    transform = transforms.Compose(
        [
            transforms.Resize((size, size)),
            transforms.ToTensor(),
        ]
    )
    return transform(img).unsqueeze(0)


def compute_inception_score(images, device, batch_size=32, splits=10):
    """
    Compute Inception Score for a list of PIL images.

    Args:
        images: List of PIL images
        device: Device to run computation on
        batch_size: Batch size for processing
        splits: Number of splits for computing mean/std

    Returns
    -------
        (mean, std) of inception score
    """
    # Load pretrained Inception v3
    inception_model = models.inception_v3(pretrained=True, transform_input=False)
    inception_model.eval()
    inception_model = inception_model.to(device)

    # Get predictions
    preds = []
    with torch.no_grad():
        for i in range(0, len(images), batch_size):
            batch = images[i : i + batch_size]
            batch_tensors = torch.stack(
                [pil_to_inception_tensor(img) for img in batch]
            ).to(device)

            # Get softmax probabilities
            pred = F.softmax(inception_model(batch_tensors), dim=1)
            preds.append(pred.cpu().numpy())

    preds = np.concatenate(preds, axis=0)

    # Compute IS
    split_scores = []
    for k in range(splits):
        part = preds[k * (len(preds) // splits) : (k + 1) * (len(preds) // splits), :]
        py = np.mean(part, axis=0)
        scores = []
        for i in range(part.shape[0]):
            pyx = part[i, :]
            scores.append(np.sum(pyx * np.log(pyx / py + 1e-10)))
        split_scores.append(np.exp(np.mean(scores)))

    del inception_model
    return np.mean(split_scores), np.std(split_scores)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Compare images generated by two LoRA models."
    )
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        default="black-forest-labs/FLUX.1-dev",
        help="Path to pretrained model or model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        help="Revision of pretrained model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--variant",
        type=str,
        default=None,
        help=(
            "Variant of the model files of the pretrained model identifier from "
            "huggingface.co/models, 'e.g.' fp16"
        ),
    )
    parser.add_argument(
        "--lora_dir",
        type=str,
        default=None,
        help="directory containing LoRA weights to load",
    )
    parser.add_argument(
        "--lora_steps",
        type=int,
        default=None,
        help="number of steps the LoRA weights have been trained on",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=["fashion"],
        default="fashion",
        help="Dataset to determine which prompts to use for image generation",
    )
    parser.add_argument(
        "--removal_dist",
        type=str,
        help="distribution for removing data",
        choices=["all", "uniform", "shapley", "datamodel", "loo", "aoi", "percentile"],
        default=None,
    )
    parser.add_argument(
        "--datamodel_alpha",
        type=float,
        help="alpha value for the datamodel removal distribution",
        default=None,
    )
    parser.add_argument(
        "--removal_seed",
        type=int,
        help="random seed for sampling from the removal distribution",
        default=0,
    )
    parser.add_argument(
        "--num_images",
        type=int,
        default=50,
        help="number of images to generate for computing model behaviors",
    )
    parser.add_argument(
        "--n_noises",
        type=int,
        default=3,
        help="number of noise samples per time step when calculating diffusion losses",
    )
    parser.add_argument(
        "--ckpt_freq",
        type=int,
        default=10,
        help="number of images before saving a checkpoint",
    )
    parser.add_argument(
        "--ckpt_path",
        type=str,
        default=None,
        help="filepath for saving the checkpoint",
    )
    parser.add_argument(
        "--img_dir",
        type=str,
        default=None,
        help="directory path for saving the generated images",
    )
    parser.add_argument(
        "--clean_up_ckpt",
        default=False,
        action="store_true",
        help="whether to clean up the checkpoint once the process is finished",
    )
    parser.add_argument(
        "--resolution",
        type=int,
        default=256,
        help="the resolution of generated image",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="seed for reproducible image generation",
    )
    parser.add_argument(
        "--db",
        type=str,
        help="path to database for saving the results",
        default=None,
        required=True,
    )
    parser.add_argument(
        "--exp_name",
        type=str,
        help="experiment name to save in the database",
        default=None,
        required=True,
    )
    parser.add_argument(
        "--no_duplicate",
        action="store_true",
        help="whether to avoid running a process that produces duplicate records",
    )
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=3.5,
    )
    parser.add_argument(
        "--cnn_feat_path",
        type=str,
        default=None,
        help="path to pre-extracted CNN features of training images",
    )
    parser.add_argument(
        "--percentile_file",
        type=str,
        default=None,
        help="Path to JSON file containing percentile indices",
    )
    parser.add_argument(
        "--percentile_value",
        type=int,
        choices=[1, 2, 3, 4, 5, 10, 15, 20, 30],
        default=10,
        help="Percentile value to use",
    )
    parser.add_argument(
        "--percentile_type",
        type=str,
        choices=["top", "bottom"],
        default="bottom",
        help="Whether to use top or bottom percentile",
    )
    parser.add_argument(
        "--percentile_mode",
        type=str,
        choices=["remove", "keep"],
        default="remove",
        help=(
            "Mode for percentile removal: 'remove'"
            "to remove indices, 'keep' to keep only those indices"
        ),
    )

    return parser.parse_args()


def main(args):
    """Main function for computing model behavior"""

    # Initialize accelerator
    accelerator = Accelerator()
    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

    # Load pipeline
    pipeline = FluxPipeline.from_pretrained(
        args.pretrained_model_name_or_path,
        revision=args.revision,
        variant=args.variant,
        torch_dtype=torch_dtype,
    )

    # Load LoRA weights if provided
    if args.lora_dir is not None:
        pipeline.load_lora_weights(
            args.lora_dir, weight_name="pytorch_lora_weights.safetensors"
        )
        print(f"Load LORA from {args.lora_dir}")

    pipeline = pipeline.to(accelerator.device)
    pipeline.set_progress_bar_config(disable=True)

    # Initialize LPIPS model for perceptual similarity
    lpips_model = lpips.LPIPS(net="alex").to(accelerator.device)
    lpips_model.eval()

    # Handle percentile-based removal: load indices from JSON file
    removal_idx_to_use = args.removal_seed
    percentile_indices_to_use = None

    if args.removal_dist == "percentile":
        if args.percentile_file is None:
            raise ValueError("--percentile_file must be specified.'")

        print(f"Loading percentile data from {args.percentile_file}")
        with open(args.percentile_file, "r") as f:
            percentile_data = json.load(f)

        # Get indices based on percentile type and value
        key = f"{args.percentile_type}_{args.percentile_value}pct_indices"
        percentile_indices = percentile_data.get(key)

        if percentile_indices is None:
            raise ValueError(f"Key '{key}' not found in {args.percentile_file}")

        print(
            f"Loaded {len(percentile_indices)} indices from "
            f"{args.percentile_type} {args.percentile_value}% percentile"
        )

        percentile_indices_to_use = percentile_indices
        removal_idx_to_use = percentile_indices

    # Load dataset and sample prompts
    create_dataset_kwargs = {
        "dataset_name": "fashion",
        "train": True,
        "removal_dist": args.removal_dist,
        "removal_idx": removal_idx_to_use,
        "datamodel_alpha": args.datamodel_alpha,
    }

    # Add percentile-specific parameters
    if args.removal_dist == "percentile":
        create_dataset_kwargs["percentile_indices"] = percentile_indices_to_use
        create_dataset_kwargs["percentile_mode"] = args.percentile_mode

    dataset, _ = create_dataset(**create_dataset_kwargs)

    # Sample prompts deterministically
    n = len(dataset["prompt"])
    if n == 0:
        raise ValueError("Dataset returned 0 captions")

    probs = torch.full((n,), 1.0 / n)
    g = torch.Generator().manual_seed(args.removal_seed)
    idx = torch.multinomial(
        probs, num_samples=args.num_images, replacement=True, generator=g
    )
    prompt_list = [dataset["prompt"][i] for i in idx.tolist()]
    filename_list = [dataset["filename"][i] for i in idx.tolist()]

    # Create output directory if needed
    if args.img_dir is not None:
        Path(args.img_dir).mkdir(parents=True, exist_ok=True)

    start_idx = 0
    images = []
    lpips_scores = []
    for i in tqdm(range(start_idx, args.num_images), desc="Generating images"):
        seed_per_image = args.seed + i
        generator = torch.Generator(device=accelerator.device).manual_seed(
            seed_per_image
        )

        # Pre-calculate prompt embeds (T5 doesn't support autocast)
        with torch.no_grad():
            prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(
                prompt_list[i], prompt_2=prompt_list[i]
            )

        with torch.no_grad():
            image = pipeline(
                prompt_embeds=prompt_embeds,
                pooled_prompt_embeds=pooled_prompt_embeds,
                guidance_scale=args.guidance_scale,
                height=args.resolution,
                width=args.resolution,
                num_inference_steps=50,
                generator=generator,
            ).images[0]

        images.append(image)

        if args.img_dir is not None:
            os.makedirs(args.img_dir, exist_ok=True)
            img_path = Path(args.img_dir) / f"img_{i:05d}.png"
            image.save(img_path)

        # Compute LPIPS with true training image
        true_img_path = os.path.join(
            DATASET_DIR, "fashion-product", "top100", filename_list[i]
        )
        if os.path.exists(true_img_path):
            with torch.no_grad():
                true_image = Image.open(true_img_path).convert("RGB")

                # LPIPS
                img_tensor = pil_to_lpips_tensor(image).to(accelerator.device)
                true_img_tensor = pil_to_lpips_tensor(true_image).to(accelerator.device)
                lpips_score = lpips_model(img_tensor, true_img_tensor).item()
                lpips_scores.append(lpips_score)
        else:
            print(f"Warning: True image not found at {true_img_path}")

        # Save image if directory is provided
        if args.img_dir is not None:
            os.makedirs(args.img_dir, exist_ok=True)
            img_path = Path(args.img_dir) / f"img_{i:05d}.png"
            image.save(img_path)

    # Compute metrics
    metrics = {
        "num_images_generated": len(images),
    }

    # LPIPS statistics (average perceptual distance to true training images)
    metrics["lpips_mean"] = float(np.mean(lpips_scores))
    metrics["lpips_std"] = float(np.std(lpips_scores))

    # MS-SSIM statistics on random pairs
    print("Computing MS-SSIM on random pairs...")
    msssim_scores = []
    np.random.seed(args.seed)
    for _ in range(args.num_images):
        # Randomly select two different images
        idx1, idx2 = np.random.choice(len(images), size=2, replace=False)
        img1 = pil_to_msssim_tensor(images[idx1], args.resolution).to(
            accelerator.device
        )
        img2 = pil_to_msssim_tensor(images[idx2], args.resolution).to(
            accelerator.device
        )

        with torch.no_grad():
            msssim_score = ms_ssim(img1, img2, data_range=1.0, size_average=True).item()
            msssim_scores.append(msssim_score)

    metrics["msssim_mean"] = float(np.mean(msssim_scores))
    metrics["msssim_std"] = float(np.std(msssim_scores))

    # Inception Score (measures quality and diversity)
    print("Computing Inception Score...")
    is_mean, is_std = compute_inception_score(images, accelerator.device)
    metrics["inception_score_mean"] = float(is_mean)
    metrics["inception_score_std"] = float(is_std)

    print("Computing Diversity Score...")
    (entropy, _, _, _, _,) = calculate_diversity_score(
        ref_image_dir_or_tensor=os.path.join(DATASET_DIR, "fashion-product", "top100"),
        generated_images_dir_or_tensor=images,
        num_cluster=20,
    )
    metrics["diversity_entropy"] = float(entropy)

    # Write results to database
    result = {
        "exp_name": args.exp_name,
        "lora_dir": args.lora_dir,
        "lora_steps": args.lora_steps,
        "dataset": args.dataset,
        "removal_dist": args.removal_dist,
        "removal_seed": args.removal_seed,
        "datamodel_alpha": args.datamodel_alpha,
        "num_images": args.num_images,
        "guidance_scale": args.guidance_scale,
        "resolution": args.resolution,
        "seed": args.seed,
        **metrics,
    }

    # Write to JSONL database
    Path(args.db).parent.mkdir(parents=True, exist_ok=True)
    with FileLock(args.db + ".lock"):
        with open(args.db, "a") as f:
            f.write(json.dumps(result) + "\n")

    print(f"Results written to {args.db}")


if __name__ == "__main__":
    args = parse_args()
    main(args)
